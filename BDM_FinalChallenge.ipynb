{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 204.2 MB 4.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 24.5 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612244 sha256=d0b874d84b9e0d67f5c1422f0f3a719fd513fdb5d72c4cb4cc8b8bc06203787f\n",
      "  Stored in directory: /Users/Meddy/Library/Caches/pip/wheels/ea/21/84/970b03913d0d6a96ef51c34c878add0de9e4ecbb7c764ea21f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a140e8d03ec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pyspark.sql.functions as sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#--------------------------------------------\n",
    "#Part 1\n",
    "#Total Number of parking Violations 2015-2019\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Street DF in Spark\n",
    "#Modify so only relevant columns are here\n",
    "#Need to modify county code so it can be matched with centerlines data\n",
    "df_s = spark.read.csv('hdfs:///tmp/bdm/nyc_parking_violation/', header = True,escape = '\"',inferSchema = 'true', multiLine = True).cache()\n",
    "\n",
    "#Drop null data\n",
    "df_s = df_s.na.drop(subset = ['House Number','Issue Date','Street Name','Violation County'])\n",
    "\n",
    "#Split Compound House numbers into 2 columns\n",
    "df_s = df_s.withColumn('HouseN1',sf.split(df_s['House Number'],\"\\-\")[0].cast(IntegerType()))\n",
    "df_s = df_s.withColumn('HouseN2',sf.split(df_s['House Number'],\"\\-\")[1].cast(IntegerType()))\n",
    "df_s = df_s.na.drop(subset = ['HouseN1'])\n",
    "\n",
    "#Select Relevant Data\n",
    "df_s = df_s.select(sf.substring(df_s['Issue Date'],7,10).alias('Year'),\\\n",
    "                   df_s['HouseN1'],df_s['HouseN2'],\\\n",
    "                  df_s['Street Name'].alias('Street_Name'),\\\n",
    "                  sf.when((df_s['Violation County']=='MAN')|(df_s['Violation County']=='MH')|(df_s['Violation County']=='MN')|(df_s['Violation County']=='NEWY')|(df_s['Violation County']=='NEW Y')|(df_s['Violation County']=='NY'),1)\\\n",
    "                  .otherwise(sf.when((df_s['Violation County']=='BRONX')|(df_s['Violation County']=='BX'),2)\\\n",
    "                  .otherwise(sf.when((df_s['Violation County']=='BK')|(df_s['Violation County']=='K')|(df_s['Violation County']=='KING')|(df_s['Violation County']=='KINGS'),3)\\\n",
    "                  .otherwise(sf.when((df_s['Violation County']=='Q')|(df_s['Violation County']=='QN')|(df_s['Violation County']=='QNS')|(df_s['Violation County']=='QU')|(df_s['Violation County']=='QUEEN'),4)\\\n",
    "                  .otherwise(sf.when((df_s['Violation County']=='R')|(df_s['Violation County']=='RICHMOND')|(df_s['Violation County']=='SI'),5))))).alias('Boro'))\n",
    "df_s = df_s.cache()\n",
    "\n",
    "#Subset 1: NonCompound House Numbers\n",
    "df_s1 = df_s.where(df_s['HouseN2'].isNull())\n",
    "df_s1 = df_s1.drop(df_s1['HouseN2'])\n",
    "df_s1 = df_s1.filter(df_s['Boro'].isNotNull())\n",
    "df_s1 = df_s1.cache()\n",
    "\n",
    "#Subset 2: Compound House Numbers\n",
    "df_s2 = df_s.where(df_s['HouseN2'].isNotNull())\n",
    "df_s2 = df_s2.filter(df_s['Boro'].isNotNull())\n",
    "df_s2 = df_s2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify Centerline data\n",
    "df_c = spark.read.csv('hdfs:///tmp/bdm/nyc_cscl.csv',header = True,inferSchema = 'true')\n",
    "\n",
    "#Convert PHYSICALID to correct data form\n",
    "df_c =df_c.withColumn('PHYSICALID',sf.regexp_replace(df_c['PHYSICALID'],\",\",\"\").cast(IntegerType()))\n",
    "\n",
    "#Following line used as precursor to running 2 join functions. Eliminates duplicates\n",
    "df_c = df_c.withColumn('FULL_STREE',sf.when(df_c['FULL_STREE']==df_c['ST_NAME'],'Medwin123').otherwise(df_c['FULL_STREE']))\n",
    "\n",
    "#Modify Low & High Columns for compounds\n",
    "df_c = df_c.withColumn('L_LOW_HN1',sf.split(df_c['L_LOW_HN'],\"\\-\")[0].cast(IntegerType()))\n",
    "df_c = df_c.withColumn('L_LOW_HN2',sf.split(df_c['L_LOW_HN'],\"\\-\")[1].cast(IntegerType()))\n",
    "df_c = df_c.withColumn('L_HIGH_HN1',sf.split(df_c['L_HIGH_HN'],\"\\-\")[0].cast(IntegerType()))\n",
    "df_c = df_c.withColumn('L_HIGH_HN2',sf.split(df_c['L_HIGH_HN'],\"\\-\")[1].cast(IntegerType()))\n",
    "\n",
    "df_c = df_c.withColumn('R_LOW_HN1',sf.split(df_c['R_LOW_HN'],\"\\-\")[0].cast(IntegerType()))\n",
    "df_c = df_c.withColumn('R_LOW_HN2',sf.split(df_c['R_LOW_HN'],\"\\-\")[1].cast(IntegerType()))\n",
    "df_c = df_c.withColumn('R_HIGH_HN1',sf.split(df_c['R_HIGH_HN'],\"\\-\")[0].cast(IntegerType()))\n",
    "df_c = df_c.withColumn('R_HIGH_HN2',sf.split(df_c['R_HIGH_HN'],\"\\-\")[1].cast(IntegerType()))\n",
    "\n",
    "\n",
    "#Select Relevant Data\n",
    "df_c = df_c.select(df_c['PHYSICALID'],df_c['FULL_STREE'],df_c['ST_NAME'],df_c['BOROCODE'],\\\n",
    "                   df_c['L_LOW_HN1'],df_c['L_LOW_HN2'],df_c['L_HIGH_HN1'],df_c['L_HIGH_HN2'],\\\n",
    "                   df_c['R_LOW_HN1'],df_c['R_LOW_HN2'],df_c['R_HIGH_HN1'],df_c['R_HIGH_HN2'])\n",
    "\n",
    "df_c = df_c.cache()\n",
    "\n",
    "#Subset 1: Noncompound House Numbers\n",
    "drop_list = ['L_LOW_HN2','L_HIGH_HN2','R_LOW_HN2','R_HIGH_HN2']\n",
    "df_c1 = df_c.where(df_c['L_LOW_HN2'].isNull())\n",
    "df_c1 = df_c1.drop(*drop_list)\n",
    "df_c1 = df_c1.cache()\n",
    "\n",
    "#Subset 2: Compound House Numbers\n",
    "df_c2 = df_c.where(df_c['L_LOW_HN2'].isNotNull())\n",
    "df_c2 = df_c2.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create conditions of join (Borough, Street, Street Segment)\n",
    "\n",
    "#Noncompounds\n",
    "#First join condition - 'Street_Name' == 'FULL_STREE'\n",
    "conda1 = [(df_s1['Boro'] == df_c1['BOROCODE']),\\\n",
    "        (sf.lower(df_s1['Street_Name'])) == sf.lower(df_c1['FULL_STREE']),\\\n",
    "         ((df_s1['HouseN1']%2==1)&(df_s1['HouseN1']>df_c1['L_LOW_HN1'])&(df_s1['HouseN1']<=df_c1['L_HIGH_HN1']))\\\n",
    "          |((df_s1['HouseN1']%2==0)&(df_s1['HouseN1']>df_c1['R_LOW_HN1'])&(df_s1['HouseN1']<=df_c1['R_HIGH_HN1']))]\n",
    "\n",
    "#Second join condition - 'Street_Name == 'ST_NAME'\n",
    "condb1 = [(df_s1['Boro'] == df_c1['BOROCODE']),\\\n",
    "        (sf.lower(df_s1['Street_Name'])) == sf.lower(df_c1['ST_NAME']),\\\n",
    "         ((df_s1['HouseN1']%2==1)&(df_s1['HouseN1']>df_c1['L_LOW_HN1'])&(df_s1['HouseN1']<=df_c1['L_HIGH_HN1']))\\\n",
    "          |((df_s1['HouseN1']%2==0)&(df_s1['HouseN1']>df_c1['R_LOW_HN1'])&(df_s1['HouseN1']<=df_c1['R_HIGH_HN1']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compounds\n",
    "#First join condition - 'Street_Name' == 'FULL_STREE'\n",
    "conda2 = [(df_s2['Boro'] == df_c2['BOROCODE']),\\\n",
    "        (sf.lower(df_s2['Street_Name'])) == sf.lower(df_c2['FULL_STREE']),\\\n",
    "         (((df_s2['HouseN1']>=df_c2['L_LOW_HN1'])&(df_s2['HouseN1']<=df_c2['L_HIGH_HN1']))\\\n",
    "          |((df_s2['HouseN1']>=df_c2['L_LOW_HN1'])&(df_s2['HouseN1']<=df_c2['L_HIGH_HN1']))\\\n",
    "          &((df_s2['HouseN2']%2==1)&(df_s2['HouseN2']>df_c2['L_LOW_HN2'])&(df_s2['HouseN2']<=df_c2['L_HIGH_HN2'])\\\n",
    "          |((df_s2['HouseN2']%2==0)&(df_s2['HouseN2']>df_c2['R_LOW_HN2'])&(df_s2['HouseN2']<=df_c2['R_HIGH_HN2']))))]\n",
    "\n",
    "#Second join condition - 'Street_Name == 'ST_NAME'\n",
    "condb2 = [(df_s2['Boro'] == df_c2['BOROCODE']),\\\n",
    "        (sf.lower(df_s2['Street_Name'])) == sf.lower(df_c2['ST_NAME']),\\\n",
    "         (((df_s2['HouseN1']>=df_c2['L_LOW_HN1'])&(df_s2['HouseN1']<=df_c2['L_HIGH_HN1']))\\\n",
    "          |((df_s2['HouseN1']>=df_c2['L_LOW_HN1'])&(df_s2['HouseN1']<=df_c2['L_HIGH_HN1']))\\\n",
    "          &((df_s2['HouseN2']%2==1)&(df_s2['HouseN2']>df_c2['L_LOW_HN2'])&(df_s2['HouseN2']<=df_c2['L_HIGH_HN2'])\\\n",
    "          |((df_s2['HouseN2']%2==0)&(df_s2['HouseN2']>df_c2['R_LOW_HN2'])&(df_s2['HouseN2']<=df_c2['R_HIGH_HN2']))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join data based on Borough, Street, Street Segment\n",
    "\n",
    "#Noncompounds\n",
    "df_j1 = df_s1.join(sf.broadcast(df_c1),conda1,how=\"outer\")\n",
    "df_j1 = df_j1.na.drop(subset = ['PHYSICALID'])\n",
    "\n",
    "df_j2 = df_s1.join(sf.broadcast(df_c1),condb1,how=\"outer\")\n",
    "df_j2 = df_j2.na.drop(subset = ['PHYSICALID'])\n",
    "\n",
    "df_n = df_j1.union(df_j2)\n",
    "df_n = df_n.select(df_n['PHYSICALID'],df_n['YEAR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compounds\n",
    "df_j3 = df_s2.join(sf.broadcast(df_c2),conda2,how=\"outer\")\n",
    "df_j3 = df_j3.na.drop(subset = ['PHYSICALID'])\n",
    "\n",
    "df_j4 = df_s2.join(sf.broadcast(df_c2),condb2,how=\"outer\")\n",
    "df_j4 = df_j4.na.drop(subset = ['PHYSICALID'])\n",
    "\n",
    "df_c = df_j3.union(df_j4)\n",
    "df_c = df_c.select(df_c['PHYSICALID'],df_c['YEAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Union Everything & Aggregate\n",
    "df_f = df_n.union(df_c)\n",
    "df_f = df_f.groupby('PHYSICALID').pivot('YEAR').count()\n",
    "df_f = df_f.orderBy('PHYSICALID')\n",
    "df_f = df_f.drop(df_f['null'])\n",
    "df_f = df_f.na.fill(0,subset = ['2015','2016','2017','2018','2019'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#--------------------------------------------\n",
    "#Part 2\n",
    "#Rate of Change over 2015-2019 (OLS)\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert DF to RDD to perform operations\n",
    "rdd = df_f.rdd.map(lambda x:(x[0],list(x[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [2015,2016,2017,2018,2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS_f(y):\n",
    "    for i in y:\n",
    "        Y = [i[1][0],i[1][1],i[1][2],i[1][3],i[1][4]]\n",
    "        slope = sm.OLS(Y,X).fit().params[0]\n",
    "        output = (i[0],(i[1][0],i[1][1],i[1][2],i[1][3],i[1][4],slope))\n",
    "    yield output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_f = rdd.mapPartitions(OLS_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-46b4d6f12fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'list_f' is not defined"
     ]
    }
   ],
   "source": [
    "list_f.coalesce(1,True).saveAsTextFile(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
